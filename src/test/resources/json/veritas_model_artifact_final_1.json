{
	"fairness_init":
		{
			"fair_metric_name": "fnr_parity",
			"perf_metric_name": "balanced_accuracy",
			"protected_features": ["gender", "race"],
			"fair_neutral_tolerance": 0.001, ## refer to the constraint of optimal fairness used to plot the purple star in Figure 3.11 on pg 101 of Veritas phase 1 report
			"num_rejected_applicants": 100, ## refer to Number of rejected applicants on pg 74 of Veritas phase 1 report
			"base_default_rate": ## refer to Base default rate (in the group) on pg 75 of Veritas phase 1 report
			{
				"gender": [0.4, 0.5], ## for the privileged & unprivileged groups respectively
				"race": [0.4, 0.5] ## for the privileged & unprivileged groups respectively
			}
		},

	"class_distribution": ## proportion breakdown
		{
			"fav_labels": 0.54,
			"unfav_labels": 0.46
		},

	"perf_metric_values":
		{
			"balanced_accuracy": [0.8, 0.06], ## metric value, confidence interval
			"f1_score": [0.77,0.04] ## metric value, confidence interval
			## ... Repeated for all the other performance metrics in our library
		},

	"calibration_curve":
		{
			"prob_true": [0, 0.2, 0.4, 0.6, 0.8, 1], ## the ground truth values split into 5 bins from 0 to 1
			"prob_pred": [0.08, 0.22, 0.38, 0.57, 0.72, 0.89], ## the mean predicted probability in each bin
			"score": 0.63 ## the brier loss score
		},

  "perf_dynamic": ## data to render the plot (Figure 3.10) on pg 96 of Veritas phase 1 report
			{
					"threshold":[0, 0.2, 0.4, 0.6, 0.8, 1],
					"perf":[0.5, 0.794, 0.798, 0.768, 0.685, 0.53],
					"selection_rate":[1, 0.89, 0.68, 0.55, 0.38, 0.08]
			},
	"correlation_matrix":
		{
			"feature_names": ["Product_Info", "gender", "Medical_History", "race", "marital"],  ## the top 20 features, only a sample provided here
			"corr_values": [[1, 0.18, 0.33, -0.25, 0.54], [0.18, 1, 0.33, -0.25, 0.54], [0.18, 0.33, 1, -0.25, 0.54], [0.18, 0.33, -0.25, 1, 0.54], [0.18, 0.33, -0.25, 0.54, 1]] ## the correlation matrix (5 x 5). Should be 20 x 20 in actual artifact.
		},

	"gender": {
		"fair_threshold": 0.13,
		"privileged": ["male"], ## can have more than 1 value
		"feature_distribution": ## proportion breakdown
		{
			"privileged_group": 0.54,
			"unprivileged_group": 0.46
		},
		"fair_metric_values":
		{
			"equal_odds": [0.18, 0.06, 0], ## metric value, confidence interval, neutral position
			"fnr_parity": [0.11,0.04, 0] ## metric value, confidence interval, neutral position
			  ## ... ## Repeated for all the other fairness metrics in our library
		},
		"fairness_conclusion": "unfair",
    "tradeoff":
		{
			"fair": [[0.04, 0.08, 0.16, 0.20, 0.24], [0.04, 0.08, 0.16, 0.20, 0.24], [0.04, 0.08, 0.16, 0.20, 0.24], [0.04, 0.08, 0.16, 0.20, 0.24], [0.04, 0.08, 0.16, 0.20, 0.24]], ## Shape of actual artifact is (500 x 500)
			"perf": [[0.736, 0.744, 0.752, 0.760, 0.768], [0.736, 0.744, 0.752, 0.760, 0.768], [0.736, 0.744, 0.752, 0.760, 0.768], [0.736, 0.744, 0.752, 0.760, 0.768], [0.736, 0.744, 0.752, 0.760, 0.768]], ## Shape of actual artifact is (500 x 500)
			"th_x": [0.3, 0.4, 0.5, 0.6, 0.7], # threshold values for x-axis, (500 x 1)
			"th_y": [[0.3], [0.4], [0.5], [0.6], [0.7]],  # threshold values for y-axis, (1 x 500)
			"max_perf_point": [0.640, 0.591, 0.80395], ## threshold_x, threshold_y, primary performance metric value
			"max_perf_single_th": [0.640, 0.640, 0.80285], ## threshold_x, threshold_y, primary performance metric value
			"max_perf_neutral_fair": [0.535, 0.694, 0.79377] ## threshold_x, threshold_y, primary performance metric value
		},

		"feature_importance":
		{
      "gender": [-0.03, -0.04, "fair to fair", "include"], ## 1st element refers to the primary performance metric difference for gender (model without gender - baseline model)
			"race": [-0.14, 0.02, "unfair to fair", "examine further"] ## 1st element refers to the primary performance metric difference for race (model without gender - baseline model)
			## there are 4 possible values for the 3rd element in the list: "fair to fair", "fair to unfair", "unfair to fair", "unfair to unfair"
			## there are 3 possible values for the 4th element in the list: "include", "exclude", "examine further"
		}
	},

	"race": { ### the same structure as gender and the values are duplicated
		"fair_threshold": 0.13,
		"privileged": ["chinese"],
		"feature_distribution":
		{
			"privileged_group": 0.54,
			"unprivileged_group": 0.46
		},
		"fair_metric_values":
		{
			"equal_odds": [0.18, 0.06, 0],
			"fnr_parity": [0.11,0.04, 0]
		},
		"fairness_conclusion": "unfair",
		"tradeoff":
		{
			"fair": [[0.04, 0.08, 0.16, 0.20, 0.24], [0.04, 0.08, 0.16, 0.20, 0.24], [0.04, 0.08, 0.16, 0.20, 0.24], [0.04, 0.08, 0.16, 0.20, 0.24], [0.04, 0.08, 0.16, 0.20, 0.24]],
			"perf": [[0.736, 0.744, 0.752, 0.760, 0.768], [0.736, 0.744, 0.752, 0.760, 0.768], [0.736, 0.744, 0.752, 0.760, 0.768], [0.736, 0.744, 0.752, 0.760, 0.768], [0.736, 0.744, 0.752, 0.760, 0.768]],
			"th_x": [0.3, 0.4, 0.5, 0.6, 0.7],
			"th_y": [[0.3], [0.4], [0.5], [0.6], [0.7]],
			"max_perf_point": [0.640, 0.591, 0.80395],
			"max_perf_single_th": [0.640, 0.640, 0.80285],
			"max_perf_neutral_fair": [0.535, 0.694, 0.79377]
		},

		"feature_importance":
		{
			"gender": [-0.03, -0.04, "fair to fair", "include"],
			"race": [-0.14, 0.02, "unfair to fair", "examine further"]
		}
	}
}
